{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6052b145-7a36-40f9-9925-65199a0afd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "215929e7-f93d-41c5-ba15-da32f9fd7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((3,3), dtype = torch.float)*10\n",
    "b = torch.rand((3,3), dtype = torch.float)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7e0494-b9e3-4643-8088-6e282de3a65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.6764, 3.3362, 7.6019],\n",
      "        [5.7203, 6.8302, 9.5461],\n",
      "        [4.3806, 8.1220, 3.1774]])\n",
      "tensor([[3.2793, 3.8742, 8.0686],\n",
      "        [6.5832, 5.3078, 8.6523],\n",
      "        [3.1943, 2.0256, 4.3613]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fca595f-0bf8-4c27-b130-b07546326498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.9557,  7.2105, 15.6705],\n",
      "        [12.3035, 12.1380, 18.1984],\n",
      "        [ 7.5749, 10.1476,  7.5387]])\n",
      "tensor([[ 2.3971, -0.5380, -0.4667],\n",
      "        [-0.8629,  1.5224,  0.8938],\n",
      "        [ 1.1864,  6.0964, -1.1839]])\n",
      "tensor([[18.6145, 12.9254, 61.3366],\n",
      "        [37.6578, 36.2536, 82.5958],\n",
      "        [13.9930, 16.4519, 13.8576]])\n",
      "tensor([[1.7310, 0.8611, 0.9422],\n",
      "        [0.8689, 1.2868, 1.1033],\n",
      "        [1.3714, 4.0097, 0.7285]])\n"
     ]
    }
   ],
   "source": [
    "print(a + b)\n",
    "print(a - b)\n",
    "print(a*b)\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5440e7c0-1de4-47bd-b645-2bb6bc289904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(54.3911)\n",
      "tensor(2.1847)\n",
      "tensor(6.0435)\n"
     ]
    }
   ],
   "source": [
    "std, mean = torch.std_mean(a)\n",
    "sum = torch.sum(a)\n",
    "print(sum)\n",
    "print(std)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51e1fddc-e3d0-4491-be24-93c6e908a6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "[[5.6763678 3.336244  7.601881 ]\n",
      " [5.7202888 6.83022   9.546087 ]\n",
      " [4.3806453 8.122     3.1774068]]\n"
     ]
    }
   ],
   "source": [
    "npc = np.array([1,2,3])\n",
    "pyc = torch.tensor(npc)\n",
    "print(pyc)\n",
    "\n",
    "npa = np.array(a)\n",
    "print(npa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d88c118a-b751-4cdf-ac6f-97ff4c0a3592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3532e+00, 6.0391e+00, 7.0622e+00, 2.5440e+00, 5.8133e+00],\n",
      "        [5.9183e+00, 2.1062e+00, 5.2148e-03, 9.1243e+00, 9.2477e+00],\n",
      "        [4.4478e-01, 8.7339e+00, 1.7303e+00, 1.3149e+00, 4.0917e+00],\n",
      "        [4.9170e+00, 1.5890e-01, 1.8801e+00, 5.2332e+00, 8.3610e+00]])\n",
      "tensor([[1.3532e+00, 6.0391e+00],\n",
      "        [7.0622e+00, 2.5440e+00],\n",
      "        [5.8133e+00, 5.9183e+00],\n",
      "        [2.1062e+00, 5.2148e-03],\n",
      "        [9.1243e+00, 9.2477e+00],\n",
      "        [4.4478e-01, 8.7339e+00],\n",
      "        [1.7303e+00, 1.3149e+00],\n",
      "        [4.0917e+00, 4.9170e+00],\n",
      "        [1.5890e-01, 1.8801e+00],\n",
      "        [5.2332e+00, 8.3610e+00]])\n",
      "tensor([[1.3532e+00, 6.0391e+00, 7.0622e+00, 2.5440e+00, 5.8133e+00, 5.9183e+00,\n",
      "         2.1062e+00, 5.2148e-03, 9.1243e+00, 9.2477e+00, 4.4478e-01, 8.7339e+00,\n",
      "         1.7303e+00, 1.3149e+00, 4.0917e+00, 4.9170e+00, 1.5890e-01, 1.8801e+00,\n",
      "         5.2332e+00, 8.3610e+00]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((4,5), dtype = torch.float)*10\n",
    "print(a)\n",
    "print(a.view((10,2)))\n",
    "a_reshaped = a.reshape((1,20))\n",
    "print(a_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b3e0947-c65f-475a-ada2-d6c4b8c749c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.1426, 2.6722, 5.8148],\n",
      "        [1.5374, 3.9404, 2.8217],\n",
      "        [8.5043, 3.7518, 0.3538]])\n",
      "tensor([[3.7322, 3.2374, 0.4156],\n",
      "        [7.1916, 8.7881, 5.4850],\n",
      "        [3.1763, 8.2534, 7.9722]])\n",
      "tensor([[8.1426, 2.6722, 5.8148],\n",
      "        [1.5374, 3.9404, 2.8217],\n",
      "        [8.5043, 3.7518, 0.3538],\n",
      "        [3.7322, 3.2374, 0.4156],\n",
      "        [7.1916, 8.7881, 5.4850],\n",
      "        [3.1763, 8.2534, 7.9722]])\n",
      "tensor([[8.1426, 2.6722, 5.8148, 3.7322, 3.2374, 0.4156],\n",
      "        [1.5374, 3.9404, 2.8217, 7.1916, 8.7881, 5.4850],\n",
      "        [8.5043, 3.7518, 0.3538, 3.1763, 8.2534, 7.9722]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3,3), dtype = torch.float)*10\n",
    "b = torch.rand((3,3), dtype = torch.float)*10\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(torch.cat((a,b), 0))\n",
    "print(torch.cat((a,b), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7525151-2a26-46a2-8118-809a6ad33924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3538)\n",
      "tensor([3.7518, 3.2374, 8.7881, 8.2534])\n"
     ]
    }
   ],
   "source": [
    "c = torch.cat((a,b), 0)\n",
    "print(c[2,2])\n",
    "print(c[2:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07e49aef-7596-482d-b429-f6c4a7ea1d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the size of the tensor you want to find the dot product of:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first tensor will be tensor([0.1062, 8.1136, 8.5954])\n",
      "the second tensor will be tensor([6.4452, 5.9358, 6.0402])\n",
      "their dot product is: 100.76303100585938\n",
      "their cross product is: tensor([ -2.0137,  54.7578, -51.6633])\n"
     ]
    }
   ],
   "source": [
    "len = int(input(\"Enter the size of the tensor you want to find the dot product of: \"))\n",
    "a = torch.rand(len, dtype = torch.float)*10\n",
    "b = torch.rand(len, dtype = torch.float)*10\n",
    "\n",
    "print(f\"the first tensor will be {a}\\nthe second tensor will be {b}\")\n",
    "print(f\"their dot product is: {torch.dot(a,b)}\")\n",
    "print(f\"their cross product is: {torch.linalg.cross(a,b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e74f109a-e09a-4695-95a9-9a74d5dbb483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c319d1aa-3cc8-4dc9-b951-580a5a553d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5163, 6.1333, 3.1005],\n",
      "        [7.1179, 6.3522, 7.8089],\n",
      "        [0.2477, 6.8485, 5.7656]])\n",
      "the normalized versions of the tensor is: \n",
      "tensor([[0.3000, 0.7784, 0.3773],\n",
      "        [0.9086, 0.8073, 1.0000],\n",
      "        [0.0000, 0.8730, 0.7298]])\n",
      "gradient of equation is: (tensor([7.]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3,3), dtype = torch.float)*10\n",
    "\n",
    "a_normalized = (a - torch.min(a))/(torch.max(a) - torch.min(a))\n",
    "\n",
    "print(a)\n",
    "print(f\"the normalized versions of the tensor is: \\n{a_normalized}\")\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad = True)\n",
    "y = pow(x,2) + 3*x + 5\n",
    "print(f\"gradient of equation is: {torch.autograd.grad(y, inputs = x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4ef24d5-b3f3-4472-a2f4-8d61251c43a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "#Implement a NN using Pytorch\n",
    "trainfile = 'mnist_train.csv'\n",
    "testfile = 'mnist_test.csv'\n",
    "\n",
    "train_samples = pd.read_csv(trainfile, header = None)\n",
    "test_samples = pd.read_csv(testfile, header = None)\n",
    "\n",
    "train_Labels = torch.tensor(train_samples.pop(0))\n",
    "test_Labels = torch.tensor(test_samples.pop(0))\n",
    "\n",
    "train_samples = torch.tensor(train_samples.values, dtype = torch.float)\n",
    "test_samples = torch.tensor(test_samples.values, dtype = torch.float)\n",
    "\n",
    "print(train_Labels[0])\n",
    "print(test_Labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1320318c-63e8-4784-9442-1ce1357484b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.Lin1 = torch.nn.Linear(784,64)\n",
    "        self.drop1 = torch.nn.Dropout(p=0.2)\n",
    "        self.Lin2 = torch.nn.Linear(64,30)\n",
    "        self.drop2 = torch.nn.Dropout(p=0.2)\n",
    "        self.Lin3 = torch.nn.Linear(30,10)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim = 1)\n",
    "        x = torch.nn.ReLU()(self.Lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = torch.nn.ReLU()(self.Lin2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.Lin3(x)\n",
    "        \n",
    "        output = torch.nn.functional.softmax(x, dim = 1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e677cc39-b48a-470d-9f72-347561dc9a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_Net = Net()\n",
    "optimizer = torch.optim.SGD(simple_Net.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform = transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform = transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = simple_Net(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print(f'  batch {i + 1} loss: {last_loss}')\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    \n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b180046-c857-4b60-b595-57308c56c6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 1.6898642497062684\n",
      "  batch 2000 loss: 1.701869018793106\n",
      "  batch 3000 loss: 1.710561738729477\n",
      "  batch 4000 loss: 1.6998432521820068\n",
      "  batch 5000 loss: 1.710086448788643\n",
      "  batch 6000 loss: 1.7103930699825287\n",
      "  batch 7000 loss: 1.7060140932798387\n",
      "  batch 8000 loss: 1.7009531149864197\n",
      "  batch 9000 loss: 1.6843030613660812\n",
      "  batch 10000 loss: 1.6924271295070648\n",
      "  batch 11000 loss: 1.7021549866199492\n",
      "  batch 12000 loss: 1.7014415531158447\n",
      "  batch 13000 loss: 1.6904117175340652\n",
      "  batch 14000 loss: 1.6942654416561127\n",
      "  batch 15000 loss: 1.6943294664621353\n",
      "Accuracy = 81.20999908447266\n",
      "LOSS train 1.6943294664621353 valid 1.6531153917312622\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.6888378084897995\n",
      "  batch 2000 loss: 1.6858132799863816\n",
      "  batch 3000 loss: 1.6939635347127915\n",
      "  batch 4000 loss: 1.6879104030132295\n",
      "  batch 5000 loss: 1.6947157834768296\n",
      "  batch 6000 loss: 1.6827511801719666\n",
      "  batch 7000 loss: 1.693055657863617\n",
      "  batch 8000 loss: 1.6800889637470244\n",
      "  batch 9000 loss: 1.68037040579319\n",
      "  batch 10000 loss: 1.6844669553041458\n",
      "  batch 11000 loss: 1.6816368275880813\n",
      "  batch 12000 loss: 1.6690011234283448\n",
      "  batch 13000 loss: 1.662923632979393\n",
      "  batch 14000 loss: 1.6715866630077363\n",
      "  batch 15000 loss: 1.6658469176292419\n",
      "Accuracy = 86.26000213623047\n",
      "LOSS train 1.6658469176292419 valid 1.6155833005905151\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.66132656955719\n",
      "  batch 2000 loss: 1.6634814568758012\n",
      "  batch 3000 loss: 1.6623085314035415\n",
      "  batch 4000 loss: 1.669239147901535\n",
      "  batch 5000 loss: 1.6704723619222641\n",
      "  batch 6000 loss: 1.6598083684444427\n",
      "  batch 7000 loss: 1.6516551409959792\n",
      "  batch 8000 loss: 1.6533585044145584\n",
      "  batch 9000 loss: 1.6673393572568893\n",
      "  batch 10000 loss: 1.6527698755264282\n",
      "  batch 11000 loss: 1.6509479510784149\n",
      "  batch 12000 loss: 1.6444317260980605\n",
      "  batch 13000 loss: 1.6463732099533082\n",
      "  batch 14000 loss: 1.6508431327342987\n",
      "  batch 15000 loss: 1.6453084431886673\n",
      "Accuracy = 88.12000274658203\n",
      "LOSS train 1.6453084431886673 valid 1.595048427581787\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.653938150882721\n",
      "  batch 2000 loss: 1.6405716588497161\n",
      "  batch 3000 loss: 1.6351949127912522\n",
      "  batch 4000 loss: 1.6440062426328659\n",
      "  batch 5000 loss: 1.6433935105800628\n",
      "  batch 6000 loss: 1.6335747749805452\n",
      "  batch 7000 loss: 1.6409612673521041\n",
      "  batch 8000 loss: 1.6440088554620742\n",
      "  batch 9000 loss: 1.638942217707634\n",
      "  batch 10000 loss: 1.6348326275348664\n",
      "  batch 11000 loss: 1.6488038663864135\n",
      "  batch 12000 loss: 1.643195857167244\n",
      "  batch 13000 loss: 1.6372637213468553\n",
      "  batch 14000 loss: 1.6273792457580567\n",
      "  batch 15000 loss: 1.6304428795576096\n",
      "Accuracy = 89.16999816894531\n",
      "LOSS train 1.6304428795576096 valid 1.582651972770691\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.6352825434207916\n",
      "  batch 2000 loss: 1.6329360049962998\n",
      "  batch 3000 loss: 1.6333471565246582\n",
      "  batch 4000 loss: 1.6293923597335815\n",
      "  batch 5000 loss: 1.6307491496801376\n",
      "  batch 6000 loss: 1.626917848944664\n",
      "  batch 7000 loss: 1.6259849317073822\n",
      "  batch 8000 loss: 1.6200402212142944\n",
      "  batch 9000 loss: 1.6338085547685623\n",
      "  batch 10000 loss: 1.633610536813736\n",
      "  batch 11000 loss: 1.6337815636396409\n",
      "  batch 12000 loss: 1.6276473959684372\n",
      "  batch 13000 loss: 1.6304435569047928\n",
      "  batch 14000 loss: 1.6255348491668702\n",
      "  batch 15000 loss: 1.6230014307498932\n",
      "Accuracy = 89.61000061035156\n",
      "LOSS train 1.6230014307498932 valid 1.576303243637085\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/number_recognition{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    simple_Net.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    simple_Net.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = simple_Net(vinputs)\n",
    "            vouts_indexes = [int(np.argmax(output)) for output in voutputs]\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            '''print(vlabels.shape)\n",
    "            print(vlabels)\n",
    "            print(voutputs.shape)\n",
    "            print(vouts_indexes)\n",
    "            print((vouts_indexes == vlabels))'''\n",
    "\n",
    "\n",
    "            correct += sum([vout == vl for vout,vl in zip(vouts_indexes,vlabels)])\n",
    "\n",
    "        accuracy = 100 * correct / len(validation_set)\n",
    "        print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(simple_Net.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "af44e2ba-6ff5-43d4-9efd-16d93014e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 8, 3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.Conv2d(8, 16, 3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(400, 30),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(30, 10),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5e99b9a0-9b69-42ce-b143-5166e49f632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform = transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform = transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print(f'  batch {i + 1} loss: {last_loss}')\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "    \n",
    "    return last_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "68ee4ecb-f2bc-45fd-8d12-46f71d661645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 2.3025808975696562\n",
      "  batch 2000 loss: 2.30177400970459\n",
      "  batch 3000 loss: 2.301076328277588\n",
      "  batch 4000 loss: 2.299415441274643\n",
      "  batch 5000 loss: 2.2958686292171477\n",
      "  batch 6000 loss: 2.274007566690445\n",
      "  batch 7000 loss: 1.9666945221424104\n",
      "  batch 8000 loss: 1.7471931159496308\n",
      "  batch 9000 loss: 1.6902977672815322\n",
      "  batch 10000 loss: 1.6780843708515167\n",
      "  batch 11000 loss: 1.656922822356224\n",
      "  batch 12000 loss: 1.6573207176923752\n",
      "  batch 13000 loss: 1.6492427184581757\n",
      "  batch 14000 loss: 1.6352473433017731\n",
      "  batch 15000 loss: 1.6358036402463914\n",
      "Accuracy = 83.70999908447266\n",
      "LOSS train 1.6358036402463914 valid 1.6270028352737427\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 1.6185956604480742\n",
      "  batch 2000 loss: 1.6249510163068772\n",
      "  batch 3000 loss: 1.6173874740600587\n",
      "  batch 4000 loss: 1.6224972891807556\n",
      "  batch 5000 loss: 1.6182767416238786\n",
      "  batch 6000 loss: 1.6253835686445237\n",
      "  batch 7000 loss: 1.613750473856926\n",
      "  batch 8000 loss: 1.6138449010848999\n",
      "  batch 9000 loss: 1.6072709685564042\n",
      "  batch 10000 loss: 1.6010993798971176\n",
      "  batch 11000 loss: 1.6046242023706436\n",
      "  batch 12000 loss: 1.6079449926614762\n",
      "  batch 13000 loss: 1.5970011403560638\n",
      "  batch 14000 loss: 1.5986346601247787\n",
      "  batch 15000 loss: 1.601869680404663\n",
      "Accuracy = 87.2300033569336\n",
      "LOSS train 1.601869680404663 valid 1.5898611545562744\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 1.6006237044334413\n",
      "  batch 2000 loss: 1.5817089225053786\n",
      "  batch 3000 loss: 1.592431129336357\n",
      "  batch 4000 loss: 1.601747297167778\n",
      "  batch 5000 loss: 1.5867440185546875\n",
      "  batch 6000 loss: 1.5909911111593247\n",
      "  batch 7000 loss: 1.5966070882081986\n",
      "  batch 8000 loss: 1.59122825050354\n",
      "  batch 9000 loss: 1.5957393999099732\n",
      "  batch 10000 loss: 1.5919535417556763\n",
      "  batch 11000 loss: 1.5828655368089677\n",
      "  batch 12000 loss: 1.5879379522800445\n",
      "  batch 13000 loss: 1.5861244904994964\n",
      "  batch 14000 loss: 1.5771325161457062\n",
      "  batch 15000 loss: 1.5891700752973557\n",
      "Accuracy = 87.91000366210938\n",
      "LOSS train 1.5891700752973557 valid 1.582578182220459\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 1.5917797671556473\n",
      "  batch 2000 loss: 1.5846044186353683\n",
      "  batch 3000 loss: 1.581332522034645\n",
      "  batch 4000 loss: 1.5817872239351272\n",
      "  batch 5000 loss: 1.5794217185974122\n",
      "  batch 6000 loss: 1.5836599348783493\n",
      "  batch 7000 loss: 1.5865692121982575\n",
      "  batch 8000 loss: 1.5767936561107636\n",
      "  batch 9000 loss: 1.5781113107204436\n",
      "  batch 10000 loss: 1.578830410838127\n",
      "  batch 11000 loss: 1.5792419644594193\n",
      "  batch 12000 loss: 1.592241515994072\n",
      "  batch 13000 loss: 1.579196509718895\n",
      "  batch 14000 loss: 1.5805753922462464\n",
      "  batch 15000 loss: 1.5804887398481369\n",
      "Accuracy = 88.69000244140625\n",
      "LOSS train 1.5804887398481369 valid 1.5746135711669922\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 1.5711671620607377\n",
      "  batch 2000 loss: 1.5793496227264405\n",
      "  batch 3000 loss: 1.5756807465553284\n",
      "  batch 4000 loss: 1.5841953344345092\n",
      "  batch 5000 loss: 1.5785037938356399\n",
      "  batch 6000 loss: 1.5825974301099777\n",
      "  batch 7000 loss: 1.5778386961221695\n",
      "  batch 8000 loss: 1.5731567747592925\n",
      "  batch 9000 loss: 1.5707381247282028\n",
      "  batch 10000 loss: 1.5778277271986008\n",
      "  batch 11000 loss: 1.57957594704628\n",
      "  batch 12000 loss: 1.5908876141309738\n",
      "  batch 13000 loss: 1.5793832764625548\n",
      "  batch 14000 loss: 1.5734503713846206\n",
      "  batch 15000 loss: 1.5692247873544694\n",
      "Accuracy = 88.55999755859375\n",
      "LOSS train 1.5692247873544694 valid 1.5752273797988892\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/number_recognition{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    net.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = net(vinputs)\n",
    "            vouts_indexes = [int(np.argmax(output)) for output in voutputs]\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "            '''print(vlabels.shape)\n",
    "            print(vlabels)\n",
    "            print(voutputs.shape)\n",
    "            print(vouts_indexes)\n",
    "            print((vouts_indexes == vlabels))'''\n",
    "\n",
    "\n",
    "            correct += sum([vout == vl for vout,vl in zip(vouts_indexes,vlabels)])\n",
    "\n",
    "        accuracy = 100 * correct / len(validation_set)\n",
    "        print(\"Accuracy = {}\".format(accuracy))\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'CNNmodel_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b427e-5d33-4f0a-8165-32e21be9b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
